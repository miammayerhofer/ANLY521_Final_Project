{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA on ACLU Bill Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/miamayerhofer/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORT PACKAGES\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import wordninja\n",
    "from nltk.corpus import stopwords\n",
    "import ast\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD IN THE DATA\n",
    "data = pd.read_csv(\"../modified_data/merged_bill_data.csv\")\n",
    "# Drop the unnamed column\n",
    "data = data.drop(data.columns[0], axis = 1)\n",
    "# Get the number of characters in each bill\n",
    "data[\"number_characters\"] = data[\"text\"].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find section of the pdf text after the \"be it enacted by\"\n",
    "def text_shortener(list_of_strings):\n",
    "    # Make all lowercase\n",
    "    long_string = list_of_strings.replace(\" \", \"\")\n",
    "    # Convert to real list\n",
    "    long_list = ast.literal_eval(long_string)\n",
    "    starting_index = 0\n",
    "    for i in range(len(long_list)):\n",
    "        # Set the starting index of the content of the bill\n",
    "        if \"beitenactedby\" in long_list[i].lower(): \n",
    "            starting_index = i\n",
    "    shortened_list = long_list[starting_index:]\n",
    "    new_string = \"\"\n",
    "    for i in range(len(shortened_list)):\n",
    "        # If there is nothing in the string\n",
    "        if len(list_of_strings[i]) == 0:\n",
    "            continue\n",
    "        # If the string just contains space characters\n",
    "        if list_of_strings[i].isspace():\n",
    "            continue\n",
    "        # If the string just contains digits\n",
    "        if list_of_strings[i].isdigit():\n",
    "            continue\n",
    "        # Remove any digits and add to the shortened bill text string\n",
    "        curr_string = re.sub(r'\\d+', '', shortened_list[i])\n",
    "        new_string = new_string + curr_string\n",
    "    if \"NewTextUnderlinedDELETEDTEXTBRACKETED\" in new_string:\n",
    "        new_string.replace(\"NewTextUnderlinedDELETEDTEXTBRACKETED\", \"\")\n",
    "    return(new_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new column with the shortened bill text using the function above\n",
    "data[\"shortened_text\"] = data[\"text\"].apply(text_shortener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ABILLFORANACTENTITLEDAnActrelatingtoschoolathleticsrecreationathleticteamsandsportsBEITENACTEDBYTHELEGISLATUREOFTHESTATEOFALASKASectionASisamendedbyaddinganewsubsectiontoreaddInthissectionsexmeansbiologicalsexSecASisamendedbyaddingnewsectionstoreadArticleDesignationofAthleticTeamsandSportsSecAthleticteamandsportdesignationaApublicschooloraprivateschoolwhosestudentsorteamscompeteagainstapublicschoolmustdesignateeachschoolsponsoredathleticteamorsportamalemenorboysteamorsportfemalewomenorgirlsteamorsportorcoeducationalormixedteamorsportbAstudentwhoparticipatesinanathleticteamorsportdesignatedfemalewomenorgirlsmustbefemalebasedontheparticipantsbiologicalsexSecComplianceprotectedAgovernmentalentitylicensingorLSAHBHBaNewTextUnderlinedDELETEDTEXTBRACKETED'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"shortened_text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Option: Word Ninja Inference Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize each line of a bill with word ninja\n",
    "def word_ninja_tokenize(string):\n",
    "    return wordninja.split(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new column of infered words\n",
    "data[\"infered_wordninja_words\"] = data[\"shortened_text\"].apply(word_ninja_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_p/d81k_b_93575z7h2220jh4cr0000gn/T/ipykernel_6376/2687247394.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"infered_wordninja_words_no_stopwords\"][i] = tokens_no_stopwords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141\n",
      "195\n"
     ]
    }
   ],
   "source": [
    "# Removing stop words\n",
    "stop_words = stopwords.words('english')\n",
    "data[\"infered_wordninja_words_no_stopwords\"] = \"\"\n",
    "for i in range(len(data[\"infered_wordninja_words\"])):\n",
    "    curr_tokens = data[\"infered_wordninja_words\"][i]\n",
    "    tokens_no_stopwords = [word for word in curr_tokens if word.lower() not in stop_words]\n",
    "    data[\"infered_wordninja_words_no_stopwords\"][i] = tokens_no_stopwords\n",
    "# Get the number of tokens with NLTK in each bill\n",
    "data[\"number_wordninja_tokens\"] = [len(token_list) for token_list in data[\"infered_wordninja_words\"]]\n",
    "data[\"number_wordninja_tokens_no_stopwords\"] = [len(token_list) for token_list in data[\"infered_wordninja_words_no_stopwords\"]]\n",
    "# Get the number of bills with less than or equal to 512 tokens\n",
    "print(len(data[(data[\"number_wordninja_tokens\"] < 512)]))\n",
    "print(len(data[(data[\"number_wordninja_tokens_no_stopwords\"] < 512)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://anubhav20057.medium.com/step-by-step-guide-abstractive-text-summarization-using-roberta-e93978234a90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import RobertaTokenizerFast, Seq2SeqTrainer, EncoderDecoderModel, TrainingArguments\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c47426aca144cd0af3f5966f65d4298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96226474e8ac4576abfba507ad7c557d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2268d3a565aa48fe9afe6d9d75c212f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97db9b53d49c4c418a2e23b158020632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TOKENIZING WITH ROBERTA\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "tokenizer.bos_token = tokenizer.cls_token\n",
    "tokenizer.eos_token = tokenizer.sep_token\n",
    "batch_size = 256  \n",
    "encoder_max_length = 40\n",
    "decoder_max_length = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_to_model_inputs(batch):\n",
    "  # tokenize the inputs and labels\n",
    "  inputs = tokenizer(batch[\"Text\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
    "  outputs = tokenizer(batch[\"Summary\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
    "\n",
    "  batch[\"input_ids\"] = inputs.input_ids\n",
    "  batch[\"attention_mask\"] = inputs.attention_mask\n",
    "  batch[\"decoder_input_ids\"] = outputs.input_ids\n",
    "  batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "  batch[\"labels\"] = outputs.input_ids.copy()\n",
    "\n",
    "  # because RoBERTa automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`. \n",
    "  # We have to make sure that the PAD token is ignored\n",
    "  batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
    "\n",
    "  return batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('anly521')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "77566a2c3ddae237484c9e2ee11aa0d0d130c667ddac0346d8d50ca9701792c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
