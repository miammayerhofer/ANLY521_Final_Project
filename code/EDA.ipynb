{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA on ACLU Bill Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/miamayerhofer/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORT PACKAGES\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import wordninja\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD IN THE DATA\n",
    "data = pd.read_csv(\"../modified_data/merged_bill_data.csv\")\n",
    "# Drop the unnamed column\n",
    "data = data.drop(data.columns[0], axis = 1)\n",
    "# Get the number of characters in each bill\n",
    "data[\"number_characters\"] = data[\"text\"].str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Option 1: NLTK word_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_p/d81k_b_93575z7h2220jh4cr0000gn/T/ipykernel_6376/1947342874.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"nltk_tokens_no_stopwords\"][i] = tokens_no_stopwords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "data[\"nltk_tokens\"] = [word_tokenize(text) for text in data[\"text\"]]\n",
    "# Removing stop words\n",
    "data[\"nltk_tokens_no_stopwords\"] = \"\"\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "for i in range(len(data[\"nltk_tokens\"])):\n",
    "    curr_tokens = data[\"nltk_tokens\"][i]\n",
    "    tokens_no_stopwords = [word for word in curr_tokens if word.lower() not in stop_words]\n",
    "    data[\"nltk_tokens_no_stopwords\"][i] = tokens_no_stopwords\n",
    "# Get the number of tokens with NLTK in each bill\n",
    "data[\"number_nltk_tokens\"] = [len(token_list) for token_list in data[\"nltk_tokens\"]]\n",
    "data[\"number_nltk_tokens_no_stopwords\"] = [len(token_list) for token_list in data[\"nltk_tokens_no_stopwords\"]]\n",
    "# Get the number of bills with less than or equal to 512 tokens\n",
    "print(len(data[(data[\"number_nltk_tokens\"] < 512)]))\n",
    "print(len(data[(data[\"number_nltk_tokens_no_stopwords\"] < 512)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Option 2: Word Ninja Inference Tokenization (after removing all whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_p/d81k_b_93575z7h2220jh4cr0000gn/T/ipykernel_6376/1212223077.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data[\"text_no_spaces\"] = data[\"text\"].str.replace(r'\\s+', '')\n",
      "/var/folders/_p/d81k_b_93575z7h2220jh4cr0000gn/T/ipykernel_6376/1212223077.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.infered_wordninja_words[i] = wordninja.split(data.text_no_spaces[i])\n"
     ]
    }
   ],
   "source": [
    "# Make a column of the text without spaces\n",
    "data[\"text_no_spaces\"] = data[\"text\"].str.replace(r'\\s+', '')\n",
    "# Make a new column of infered words\n",
    "data[\"infered_wordninja_words\"] = \"\"\n",
    "# Infer the words from this new column\n",
    "for i in range(len(data.text_no_spaces)):\n",
    "    data.infered_wordninja_words[i] = wordninja.split(data.text_no_spaces[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n",
      "150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_p/d81k_b_93575z7h2220jh4cr0000gn/T/ipykernel_6376/83215197.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"infered_wordninja_words_no_stopwords\"][i] = tokens_no_stopwords\n"
     ]
    }
   ],
   "source": [
    "# Removing stop words\n",
    "data[\"infered_wordninja_words_no_stopwords\"] = \"\"\n",
    "for i in range(len(data[\"infered_wordninja_words\"])):\n",
    "    curr_tokens = data[\"infered_wordninja_words\"][i]\n",
    "    tokens_no_stopwords = [word for word in curr_tokens if word.lower() not in stop_words]\n",
    "    data[\"infered_wordninja_words_no_stopwords\"][i] = tokens_no_stopwords\n",
    "# Get the number of tokens with NLTK in each bill\n",
    "data[\"number_wordninja_tokens\"] = [len(token_list) for token_list in data[\"infered_wordninja_words\"]]\n",
    "data[\"number_wordninja_tokens_no_stopwords\"] = [len(token_list) for token_list in data[\"infered_wordninja_words_no_stopwords\"]]\n",
    "# Get the number of bills with less than or equal to 512 tokens\n",
    "print(len(data[(data[\"number_wordninja_tokens\"] < 512)]))\n",
    "print(len(data[(data[\"number_wordninja_tokens_no_stopwords\"] < 512)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://anubhav20057.medium.com/step-by-step-guide-abstractive-text-summarization-using-roberta-e93978234a90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import RobertaTokenizerFast, Seq2SeqTrainer, EncoderDecoderModel, TrainingArguments\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c47426aca144cd0af3f5966f65d4298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96226474e8ac4576abfba507ad7c557d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2268d3a565aa48fe9afe6d9d75c212f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97db9b53d49c4c418a2e23b158020632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TOKENIZING WITH ROBERTA\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "tokenizer.bos_token = tokenizer.cls_token\n",
    "tokenizer.eos_token = tokenizer.sep_token\n",
    "batch_size = 256  \n",
    "encoder_max_length = 40\n",
    "decoder_max_length = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_to_model_inputs(batch):\n",
    "  # tokenize the inputs and labels\n",
    "  inputs = tokenizer(batch[\"Text\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
    "  outputs = tokenizer(batch[\"Summary\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
    "\n",
    "  batch[\"input_ids\"] = inputs.input_ids\n",
    "  batch[\"attention_mask\"] = inputs.attention_mask\n",
    "  batch[\"decoder_input_ids\"] = outputs.input_ids\n",
    "  batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "  batch[\"labels\"] = outputs.input_ids.copy()\n",
    "\n",
    "  # because RoBERTa automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`. \n",
    "  # We have to make sure that the PAD token is ignored\n",
    "  batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
    "\n",
    "  return batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('anly521')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "77566a2c3ddae237484c9e2ee11aa0d0d130c667ddac0346d8d50ca9701792c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
